{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73898187",
   "metadata": {},
   "source": [
    "# Face_Blur (Code by Jaechan Jo)\n",
    "### - feature_inversion method\n",
    " - Conserving original face feature map as much as possible, Blur the face\n",
    "\n",
    "### - Structure (1th draft)\n",
    " - Face-detector + Blur-feature_inversion\n",
    " - yolo_v5_face + squeeznet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c192bf3",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a547987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import PIL\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "from imageio import imread\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "from cs231n.image_utils import SQUEEZENET_MEAN, SQUEEZENET_STD\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdec5275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpu or cpu\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.FloatTensor\n",
    "    \n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd40dd2",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    " - img_dir : Directory of image dataset\n",
    " - img_txt : Directory of image bbox coordinate txt\n",
    " - save_dir : Directory of saving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cb89a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "\n",
    "img_dir = '/home/face_mosaic/yolo_v5+feature_inversion/face_data/crop/'\n",
    "img_txt = '/home/face_mosaic/yolo_v5+feature_inversion/face_data/result/crop/'\n",
    "save_dir = '/home/face_mosaic/yolo_v5+feature_inversion/face_data/crop/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239582d1",
   "metadata": {},
   "source": [
    "# Face-Detection\n",
    "### - Yolo_v5_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ca87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bounding box coordinate (x1, y1, x2, y2)\n",
    "\n",
    "!python ./yolov5_face_detection/test_widerface.py \\\n",
    "--weight ./yolov5_face_detection/weights/face_l.pt \\\n",
    "--img-size 640 \\\n",
    "--dataset_folder ./face_data/crop/ \\\n",
    "--folder_pict ./face_data/dir/crop_dir.txt \\\n",
    "--save_folder ./face_data/result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724db44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop bbox & save\n",
    "\n",
    "def crop_face(img_dir, img_txt, save_dir):\n",
    "    img_list = os.listdir(img_dir)\n",
    "    txt_list = os.listdir(img_txt)\n",
    "    boxes = []\n",
    "            \n",
    "    for txt in txt_list:\n",
    "        txt_path = img_txt + txt\n",
    "        with open(txt_path, mode='r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for i  in range(len(lines)):\n",
    "                lines[i]  = lines[i].strip('\\n')\n",
    "\n",
    "            obj = {'img_name':lines[0], 'obj_num':int(lines[1])}\n",
    "\n",
    "            box_list = []\n",
    "            for i in range(2, len(lines)):\n",
    "                box_el = []\n",
    "                x1, y1, w, h, score = lines[i].split(' ')\n",
    "                box_el.append(int(x1))\n",
    "                box_el.append(int(y1))\n",
    "                box_el.append(int(w))\n",
    "                box_el.append(int(h))\n",
    "                box_el.append(float(score))\n",
    "                box_list.append(box_el)\n",
    "\n",
    "            obj['box'] = box_list\n",
    "            boxes.append(obj)\n",
    "\n",
    "    print('boxes : ', boxes)\n",
    "    print('img_list : ', img_list)\n",
    "\n",
    "    for img in img_list:\n",
    "        img_path = img_dir + img\n",
    "        img_name, _ = img.split('.')\n",
    "\n",
    "        image = PIL.Image.open(img_path)\n",
    "        copy_image = image.copy()\n",
    "\n",
    "        obj = next((item for item in boxes if item['img_name'] == img_name), None)\n",
    "        for i in range(0, obj['obj_num']):\n",
    "            if obj['box'][i][4] >= 0.5:\n",
    "                x1 = obj['box'][i][0]\n",
    "                y1 = obj['box'][i][1]\n",
    "                x2 = obj['box'][i][0] + obj['box'][i][2]\n",
    "                y2 = obj['box'][i][1] + obj['box'][i][3]\n",
    "                cropped_image = T.functional.crop(copy_image, top=y1, left= x1, height= y2-y1, width= x2-x1)\n",
    "                cropped_image.save(save_dir+img_name+f'_crop{i+1}.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39471c",
   "metadata": {},
   "source": [
    "# Face-Blur\n",
    "### - Squeeznet_feature_inverison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, size=512):\n",
    "    transform = T.Compose([\n",
    "        T.Scale(size),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=SQUEEZENET_MEAN.tolist(), # 이부분은 각각 이미지에 맞게 평균,\n",
    "                    #표준편차를 구할 수 있도록 코드를 일반화 시켜주도록 한다.\n",
    "                    std=SQUEEZENET_STD.tolist()),\n",
    "        T.Lambda(lambda x: x[None]),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def deprocess(img):\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda x: x[0]),\n",
    "        \n",
    "        # Unnormalize 표준편차를 곱해주고, 평균을 더해주는 과정\n",
    "        T.Normalize(mean=[0, 0, 0], std=[1.0 / s for s in SQUEEZENET_STD.tolist()]),\n",
    "        T.Normalize(mean=[-m for m in SQUEEZENET_MEAN.tolist()], std=[1, 1, 1]),\n",
    "        T.Lambda(rescale),\n",
    "        T.ToPILImage(),\n",
    "    ])\n",
    "    return transform(img)\n",
    "\n",
    "def rescale(x): # MINMAX Standardization\n",
    "    low, high = x.min(), x.max()\n",
    "    x_rescaled = (x - low) / (high - low)\n",
    "    return x_rescaled\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def features_from_img(imgpath, imgsize):\n",
    "    img = preprocess(PIL.Image.open(imgpath), size=imgsize)\n",
    "    img_var = Variable(img.type(dtype))\n",
    "    return extract_features(img_var, cnn), img_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003607b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained SqueezeNet model. (imagenet pretrained)\n",
    "cnn = torchvision.models.squeezenet1_1(pretrained=True).features\n",
    "cnn.type(dtype)\n",
    "cnn.to(device)\n",
    "\n",
    "# We don't want to train the model any further, so we don't want PyTorch to waste computation \n",
    "# computing gradients on parameters we're never going to update.\n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False #학습을 원한다면 켜고 시작해야!\n",
    "\n",
    "# We provide this helper code which takes an image, a model (cnn), and returns a list of\n",
    "# feature maps, one per layer.\n",
    "def extract_features(x, cnn):\n",
    "    \"\"\"\n",
    "    Use the CNN to extract features from the input image x.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A PyTorch Variable of shape (N, C, H, W) holding a minibatch of images that\n",
    "      will be fed to the CNN.\n",
    "    - cnn: A PyTorch model that we will use to extract features.\n",
    "    \n",
    "    Returns:\n",
    "    - features: A list of feature for the input images x extracted using the cnn model.\n",
    "      features[i] is a PyTorch Variable of shape (N, C_i, H_i, W_i); recall that features\n",
    "      from different layers of the network may have different numbers of channels (C_i) and\n",
    "      spatial dimensions (H_i, W_i).\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    prev_feat = x.to(device)\n",
    "    for i, module in enumerate(cnn._modules.values()):\n",
    "        next_feat = module(prev_feat)\n",
    "        features.append(next_feat)\n",
    "        prev_feat = next_feat\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2febe1",
   "metadata": {},
   "source": [
    "# Face_Loss\n",
    "### - Feature_inversion equation\n",
    "\n",
    "![Feature Inversion Equation](./exp_img/feature_inversion_eq.png \"Feature Inversion Equation\")\n",
    "\n",
    " - l번째 original face와 output(random) image feature map을 유사하게 합니다.\n",
    " - 파란색 원: blur의 정도를 조절하는 hyper parameters\n",
    "     - layer(l) : 유사하게 할 feature map의 층, 깊을 수록 blur\n",
    "     - blur_weight : 값이 클수록 유사하게 하는 학습이 약해진다, 클수록 blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dab5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_loss(blur_weight, starting_img, target_face):\n",
    "    \"\"\"\n",
    "    Compute the content loss for style transfer.\n",
    "    \n",
    "    Inputs:\n",
    "    - blur_weight: 비식별화 정도; 숫자가 커지면 커질수록 학습이 약해져,비식별화 정도가 강해진다\n",
    "    - crop_face : 비식별화하고자 하는 얼굴 이미지만 가린 전체 이미지 feature ; this is a PyTorch Tensor of shape\n",
    "      (1, C_l, H_l, W_l).\n",
    "    - original_face : 비식별화하고자 하는 얼굴 이미지가 포함된 전체 이미지 feature ; Tensor with shape (1, C_l, H_l, W_l).\n",
    "    \n",
    "    Returns:\n",
    "    - scalar feature_loss\n",
    "    \"\"\"\n",
    "    feature_loss = (1/blur_weight) * torch.sum((torch.pow(starting_img - target_face, 2)))\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    cosine_distance = 1 - cos(torch.flatten(starting_img), torch.flatten(target_face))\n",
    "    return feature_loss, cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6608c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_feature_inversion(original_face, target_layer, image_size, \n",
    "                           blur_weight,  init_random = False):\n",
    "    \"\"\"\n",
    "    Run style transfer!\n",
    "    \n",
    "    Inputs:\n",
    "    - original_face: an image of what is trying to blur face\n",
    "\n",
    "    - image_size: size of smallest image dimension (used for content loss and generated image)\n",
    "\n",
    "    - content_layer: layer to use for content loss\n",
    "    - content_weight: weighting on content loss\n",
    "\n",
    "    - init_random: initialize the starting image to uniform random noise\n",
    "    \"\"\"\n",
    "    print(f\"target_layer: {target_layer}/ blur_weight: {blur_weight}\")\n",
    "    \n",
    "    # Extract features for the content image\n",
    "    face_img = preprocess(PIL.Image.open(original_face), size=image_size)\n",
    "    face_img = face_img.type(dtype)\n",
    "    feats = extract_features(face_img, cnn)\n",
    "    feature_target = feats[target_layer].clone()\n",
    "#     # Initialize output image to content image or nois\n",
    "#     if init_random:\n",
    "#         img = torch.Tensor(*content_img_var.size()).uniform_(0, 1).type(dtype)\n",
    "#     else:\n",
    "#         img = content_img.clone().type(dtype)\n",
    "    \n",
    "    # start img - uniform random ver. / black ver.(해보자!!)\n",
    "    s_img = torch.Tensor(*face_img.size()).uniform_(0, 1).type(dtype)\n",
    "    s_img.requires_grad_()\n",
    "#     s_img = np.transpose(torch.squeeze(start_img), (1,2,0))\n",
    "\n",
    "    # Set up optimization hyperparameters\n",
    "    initial_lr = 3.0\n",
    "    decayed_lr = 0.1\n",
    "    decay_lr_at = 180\n",
    "\n",
    "    # Note that we are optimizing the pixel values of the image by passing\n",
    "    # in the s_img_var Torch variable, whose requires_grad flag is set to True\n",
    "    optimizer = torch.optim.Adam([s_img], lr=initial_lr)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].axis('off')\n",
    "    axarr[1].axis('off')\n",
    "    axarr[0].set_title('Original Face.')\n",
    "    axarr[1].set_title('Starting img.')\n",
    "    axarr[0].imshow(deprocess(face_img.cpu()))\n",
    "    axarr[1].imshow(deprocess(s_img.data.cpu()))\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    \n",
    "#     content_image = 'styles/tubingen.jpg'\n",
    "# image_size =  192\n",
    "# content_layer = 3\n",
    "# content_weight = 6e-2\n",
    "# # cnn(squeezenet1_1(pretrained=True))에서 추출한 feature, require_grad=True인 데이터모드\n",
    "# c_feats, content_img_var = features_from_img(content_image, image_size)\n",
    "# print([c_feats[i].size() for i in range(4)])\n",
    "\n",
    "# bad_img = Variable(torch.zeros(*content_img_var.data.size()))\n",
    "# feats = extract_features(bad_img, cnn)\n",
    "# print([feats[i].size() for i in range(4)])\n",
    "\n",
    "# f_l, c_d = face_loss(content_weight, c_feats[content_layer], feats[content_layer])\n",
    "# # print(face_loss(content_weight, c_feats[content_layer], feats[content_layer]))\n",
    "# print(f_l, c_d)\n",
    "    \n",
    "    \n",
    "    for t in range(10000):\n",
    "        if t < 190:\n",
    "            s_img.data.clamp_(-1.5, 1.5)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        feats = extract_features(s_img, cnn)\n",
    "        \n",
    "        # Compute loss\n",
    "        feature_loss, cosine_distance = face_loss(blur_weight, feats[target_layer], feature_target)\n",
    "        loss = feature_loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient descents on our image values (180번 iter에 lr를 0.1씩 decay 시키면서 학습)\n",
    "        if t == decay_lr_at:\n",
    "            optimizer = torch.optim.Adam([s_img], lr=decayed_lr)\n",
    "        optimizer.step()\n",
    "\n",
    "        if t % 1000 == 0:\n",
    "            print('Iteration {}'.format(t))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(deprocess(s_img.data.cpu()))\n",
    "            plt.show()\n",
    "            cos = torch.nn.CosineSimilarity(dim=0)\n",
    "            cosine_similarity = cos(torch.flatten(s_img), torch.flatten(face_img))\n",
    "            print(f'원본 이미지와 유사도: {cosine_similarity}')\n",
    "    print('Iteration {}'.format(t))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(deprocess(s_img.data.cpu()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ab53c",
   "metadata": {},
   "source": [
    "# Synthesize\n",
    "### - original image + cropped blur face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본이미지, 크롭 이미지 불러오기\n",
    "original_image = './face_data/img/face1.jpg'\n",
    "cropped_blur_image = './face_data/result/result/face1_crop1_result10000.jpg'\n",
    "\n",
    "o_img = cv2.imread(original_image)\n",
    "ori_img = cv2.cvtColor(o_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "c_img = cv2.imread(cropped_blur_image)\n",
    "crop_img = cv2.cvtColor(c_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(6,12))\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(ori_img)\n",
    "ax[1].imshow(crop_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1baa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box 좌표 값 불러오기\n",
    "txt_path = './face_data/result/face_data/face1.txt'\n",
    "boxes = []\n",
    "with open(txt_path, mode='r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for i  in range(len(lines)):\n",
    "                lines[i]  = lines[i].strip('\\n')\n",
    "\n",
    "            obj = {'img_name':lines[0], 'obj_num':int(lines[1])}\n",
    "\n",
    "            box_list = []\n",
    "            for i in range(2, len(lines)):\n",
    "                box_el = []\n",
    "                x1, y1, w, h, score = lines[i].split(' ')\n",
    "                box_el.append(int(x1))\n",
    "                box_el.append(int(y1))\n",
    "                box_el.append(int(w))\n",
    "                box_el.append(int(h))\n",
    "                box_el.append(float(score))\n",
    "                box_list.append(box_el)\n",
    "\n",
    "            obj['box'] = box_list\n",
    "            boxes.append(obj)\n",
    "\n",
    "print('boxes : ', boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ebcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box 부분 값을 비우고, blur 사진 크기 맞춰서 합성하기\n",
    "new_ori = ori_img.copy()\n",
    "\n",
    "save_dir = './face_data/result/synthesis/'\n",
    "\n",
    "obj = [item for item in boxes if item['img_name'] == 'face1']\n",
    "for i in range(0, obj[0]['obj_num']):\n",
    "    if obj[0]['box'][i][4] >= 0.5:\n",
    "        x1 = obj[0]['box'][i][0]\n",
    "        y1 = obj[0]['box'][i][1]\n",
    "        x2 = obj[0]['box'][i][0] + obj[0]['box'][i][2]\n",
    "        y2 = obj[0]['box'][i][1] + obj[0]['box'][i][3]\n",
    "        #To zero(black) in bbox\n",
    "        new_ori[y1:y2, x1:x2, :] -= new_ori[y1:y2, x1:x2, :]\n",
    "        \n",
    "        #crop_img resize for syntheis\n",
    "        pil_c_img=PIL.Image.fromarray(crop_img)\n",
    "        rs_c_img = T.functional.resize(pil_c_img, size=[y2-y1, x2-x1])\n",
    "        r_crop_img = np.array(rs_c_img)\n",
    "        \n",
    "        #Synthesize cropped image to original\n",
    "        new_ori[y1:y2, x1:x2, :] += r_crop_img\n",
    "        \n",
    "#         cropped_image.save(save_dir+img_name+f'_crop{i+1}.jpg', 'JPEG')\n",
    "\n",
    "plt.imshow(new_ori)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#save\n",
    "PIL.Image.fromarray(new_ori).save(save_dir+'test.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e0e3c",
   "metadata": {},
   "source": [
    "# How much similiar between original and blur in feature map?\n",
    "### - Cosine similiarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similiarity\n",
    "cos = torch.nn.CosineSimilarity(dim=0)\n",
    "cs = cos(torch.flatten(torch.Tensor(new_ori)), torch.flatten(torch.Tensor(ori_img)))\n",
    "print(cs)\n",
    "\n",
    "# Final Image Show\n",
    "fig, axes = plt.subplots(1,2,figsize=(6,12))\n",
    "ax = axes.flatten()\n",
    "ax[0].imshow(new_ori)\n",
    "ax[0].axis('off')\n",
    "ax[1].imshow(ori_img)\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7cc14f",
   "metadata": {},
   "source": [
    "![Feature Inversion Result](./exp_img/result.png \"Feature Inversion Result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
